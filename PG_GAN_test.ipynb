{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape, Input, Lambda\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, Cropping2D\n",
    "from keras.layers import LeakyReLU, Dropout, UpSampling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model\n",
    "from keras.backend import int_shape, tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG_GAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    def discriminator(self):\n",
    "        def d_block(x,n_features,kernel,stride):\n",
    "            x = Conv2D(n_features, kernel, strides=stride, padding = 'same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = Dropout(.2)(x)\n",
    "            return x\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        \n",
    "        depth = 64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        \n",
    "        x = Input(shape=input_shape)\n",
    "        y = d_block(x,depth*1,5,2)\n",
    "        y = d_block(y,depth*2,5,2)\n",
    "        y = d_block(y,depth*4,5,2)\n",
    "        y = d_block(y,depth*8,5,1)\n",
    "        \n",
    "        y = Flatten(name='Flatten')(y)\n",
    "        y = Dense(1,activation = 'sigmoid')(y)\n",
    "        self.D = Model(x,y)\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "    \n",
    "    def generator(self):\n",
    "        def g_block(x,n_features,kernel,stride):\n",
    "            x = UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "            x = Conv2D(n_features, kernel, strides=stride, padding='same')(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(momentum=0.9)(x)\n",
    "            return x\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        depth = 64\n",
    "        dim = self.discriminator().get_layer('Flatten').input_shape[1]\n",
    "        x = Input(shape=(100,))\n",
    "        y = Dense(dim*dim*depth*8)(x)\n",
    "        y = LeakyReLU(alpha=0.2)(y)\n",
    "        y = Reshape((dim, dim, depth*8))(y)\n",
    "        y = BatchNormalization(momentum=0.9)(y)\n",
    "\n",
    "        y = g_block(y,depth*4,5,1)\n",
    "        y = g_block(y,depth*2,5,1)\n",
    "        y = g_block(y,depth,5,1)\n",
    "        \n",
    "\n",
    "        y = Conv2D(1, 5, padding='same')(y)\n",
    "        y = Activation('tanh', name='Tanh')(y)\n",
    "        crop = int((int_shape(y)[1]-self.D.input_shape[1])/2)\n",
    "        y = Cropping2D(cropping=((crop,crop),(crop,crop)), name = 'Crop2D')(y)\n",
    "        self.G = Model(x,y)\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = Adam(lr=0.0002,beta_1=0.5, decay=0)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = Adam(lr=0.0002,beta_1=0.5, decay=0)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        discriminator = self.discriminator()#.trainable=False\n",
    "        self.AM.add(discriminator)\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        self.AM.summary()\n",
    "        return self.AM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        (self.x_train, _), (_, _) = mnist.load_data()\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\n",
    "        \tself.img_cols, 1).astype(np.float32)/255*2-1\n",
    "\n",
    "        self.PGGAN = PG_GAN()\n",
    "        self.discriminator =  self.PGGAN.discriminator_model()\n",
    "        self.adversarial = self.PGGAN.adversarial_model()\n",
    "        self.generator = self.PGGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8192)              827392    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 256)         3277056   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 128)       819328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 64)        204864    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 1)         1601      \n",
      "_________________________________________________________________\n",
      "Tanh (Activation)            (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Crop2D (Cropping2D)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 5,134,081\n",
      "Trainable params: 5,132,161\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_3 (Model)              (None, 28, 28, 1)         5134081   \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 4311553   \n",
      "=================================================================\n",
      "Total params: 9,445,634\n",
      "Trainable params: 9,443,714\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.695731, acc: 0.492188]  [A loss: 0.599515, acc: 1.000000]\n",
      "1: [D loss: 0.754046, acc: 0.500000]  [A loss: 0.502768, acc: 1.000000]\n",
      "2: [D loss: 0.881404, acc: 0.500000]  [A loss: 0.473083, acc: 1.000000]\n",
      "3: [D loss: 0.911918, acc: 0.500000]  [A loss: 0.469385, acc: 1.000000]\n",
      "4: [D loss: 0.873832, acc: 0.500000]  [A loss: 0.463375, acc: 1.000000]\n",
      "5: [D loss: 0.838060, acc: 0.500000]  [A loss: 0.447516, acc: 1.000000]\n",
      "6: [D loss: 0.825952, acc: 0.500000]  [A loss: 0.422548, acc: 1.000000]\n",
      "7: [D loss: 0.888840, acc: 0.500000]  [A loss: 0.414113, acc: 1.000000]\n",
      "8: [D loss: 0.900132, acc: 0.500000]  [A loss: 0.406242, acc: 1.000000]\n",
      "9: [D loss: 0.940419, acc: 0.500000]  [A loss: 0.409366, acc: 1.000000]\n",
      "10: [D loss: 0.930084, acc: 0.500000]  [A loss: 0.414445, acc: 1.000000]\n",
      "11: [D loss: 0.897386, acc: 0.500000]  [A loss: 0.409417, acc: 1.000000]\n",
      "12: [D loss: 0.922981, acc: 0.500000]  [A loss: 0.404484, acc: 1.000000]\n",
      "13: [D loss: 0.923181, acc: 0.500000]  [A loss: 0.398608, acc: 1.000000]\n",
      "14: [D loss: 0.961094, acc: 0.500000]  [A loss: 0.400955, acc: 1.000000]\n",
      "15: [D loss: 0.942765, acc: 0.500000]  [A loss: 0.399135, acc: 1.000000]\n",
      "16: [D loss: 0.918061, acc: 0.500000]  [A loss: 0.391523, acc: 1.000000]\n",
      "17: [D loss: 0.936985, acc: 0.500000]  [A loss: 0.383292, acc: 1.000000]\n",
      "18: [D loss: 0.962962, acc: 0.500000]  [A loss: 0.383116, acc: 1.000000]\n",
      "19: [D loss: 0.955052, acc: 0.500000]  [A loss: 0.379889, acc: 1.000000]\n",
      "20: [D loss: 0.961524, acc: 0.500000]  [A loss: 0.379203, acc: 1.000000]\n",
      "21: [D loss: 0.971857, acc: 0.500000]  [A loss: 0.378345, acc: 1.000000]\n",
      "22: [D loss: 0.965029, acc: 0.500000]  [A loss: 0.377131, acc: 1.000000]\n",
      "23: [D loss: 0.962957, acc: 0.500000]  [A loss: 0.371602, acc: 1.000000]\n",
      "24: [D loss: 0.971807, acc: 0.500000]  [A loss: 0.368825, acc: 1.000000]\n",
      "25: [D loss: 0.981663, acc: 0.500000]  [A loss: 0.367497, acc: 1.000000]\n",
      "26: [D loss: 0.969963, acc: 0.500000]  [A loss: 0.364293, acc: 1.000000]\n",
      "27: [D loss: 0.977346, acc: 0.500000]  [A loss: 0.362593, acc: 1.000000]\n",
      "28: [D loss: 0.979105, acc: 0.500000]  [A loss: 0.360966, acc: 1.000000]\n",
      "29: [D loss: 0.977422, acc: 0.500000]  [A loss: 0.359925, acc: 1.000000]\n",
      "30: [D loss: 0.969013, acc: 0.500000]  [A loss: 0.356217, acc: 1.000000]\n",
      "31: [D loss: 0.973899, acc: 0.500000]  [A loss: 0.353278, acc: 1.000000]\n",
      "32: [D loss: 0.971517, acc: 0.500000]  [A loss: 0.351937, acc: 1.000000]\n",
      "33: [D loss: 0.972633, acc: 0.500000]  [A loss: 0.349623, acc: 1.000000]\n",
      "34: [D loss: 0.969602, acc: 0.500000]  [A loss: 0.349481, acc: 1.000000]\n",
      "35: [D loss: 0.974850, acc: 0.500000]  [A loss: 0.349757, acc: 1.000000]\n",
      "36: [D loss: 0.960956, acc: 0.500000]  [A loss: 0.347602, acc: 1.000000]\n",
      "37: [D loss: 0.961526, acc: 0.500000]  [A loss: 0.345064, acc: 1.000000]\n",
      "38: [D loss: 0.966747, acc: 0.500000]  [A loss: 0.345926, acc: 1.000000]\n",
      "39: [D loss: 0.960787, acc: 0.500000]  [A loss: 0.343124, acc: 1.000000]\n",
      "40: [D loss: 0.967157, acc: 0.500000]  [A loss: 0.342675, acc: 1.000000]\n",
      "41: [D loss: 0.967788, acc: 0.500000]  [A loss: 0.342598, acc: 1.000000]\n",
      "42: [D loss: 0.973742, acc: 0.500000]  [A loss: 0.344263, acc: 1.000000]\n",
      "43: [D loss: 0.968386, acc: 0.500000]  [A loss: 0.343066, acc: 1.000000]\n",
      "44: [D loss: 0.962512, acc: 0.500000]  [A loss: 0.342127, acc: 1.000000]\n",
      "45: [D loss: 0.964500, acc: 0.500000]  [A loss: 0.335342, acc: 1.000000]\n",
      "46: [D loss: 0.971075, acc: 0.500000]  [A loss: 0.332097, acc: 1.000000]\n",
      "47: [D loss: 0.972346, acc: 0.500000]  [A loss: 0.325795, acc: 1.000000]\n",
      "48: [D loss: 0.984540, acc: 0.500000]  [A loss: 0.327853, acc: 1.000000]\n",
      "49: [D loss: 0.977531, acc: 0.500000]  [A loss: 0.325245, acc: 1.000000]\n",
      "50: [D loss: 0.981565, acc: 0.500000]  [A loss: 0.325839, acc: 1.000000]\n",
      "51: [D loss: 0.977538, acc: 0.500000]  [A loss: 0.326644, acc: 1.000000]\n",
      "52: [D loss: 0.976512, acc: 0.500000]  [A loss: 0.326601, acc: 1.000000]\n",
      "53: [D loss: 0.975721, acc: 0.500000]  [A loss: 0.323896, acc: 1.000000]\n",
      "54: [D loss: 0.973842, acc: 0.500000]  [A loss: 0.321823, acc: 1.000000]\n",
      "55: [D loss: 0.975341, acc: 0.500000]  [A loss: 0.321183, acc: 1.000000]\n",
      "56: [D loss: 0.974254, acc: 0.500000]  [A loss: 0.319461, acc: 1.000000]\n",
      "57: [D loss: 0.976611, acc: 0.500000]  [A loss: 0.317680, acc: 1.000000]\n",
      "58: [D loss: 0.981076, acc: 0.500000]  [A loss: 0.318975, acc: 1.000000]\n",
      "59: [D loss: 0.987583, acc: 0.500000]  [A loss: 0.319811, acc: 1.000000]\n",
      "60: [D loss: 0.975901, acc: 0.500000]  [A loss: 0.323089, acc: 1.000000]\n",
      "61: [D loss: 0.964653, acc: 0.500000]  [A loss: 0.319454, acc: 1.000000]\n",
      "62: [D loss: 0.968193, acc: 0.500000]  [A loss: 0.318192, acc: 1.000000]\n",
      "63: [D loss: 0.946238, acc: 0.500000]  [A loss: 0.322507, acc: 1.000000]\n",
      "64: [D loss: 0.994937, acc: 0.500000]  [A loss: 0.312987, acc: 1.000000]\n",
      "65: [D loss: 0.991056, acc: 0.500000]  [A loss: 0.321280, acc: 1.000000]\n",
      "66: [D loss: 0.978497, acc: 0.500000]  [A loss: 0.315912, acc: 1.000000]\n",
      "67: [D loss: 0.984071, acc: 0.500000]  [A loss: 0.313207, acc: 1.000000]\n",
      "68: [D loss: 0.981707, acc: 0.500000]  [A loss: 0.311300, acc: 1.000000]\n",
      "69: [D loss: 0.988693, acc: 0.500000]  [A loss: 0.305872, acc: 1.000000]\n",
      "70: [D loss: 0.996055, acc: 0.500000]  [A loss: 0.303920, acc: 1.000000]\n",
      "71: [D loss: 1.008347, acc: 0.500000]  [A loss: 0.312862, acc: 1.000000]\n",
      "72: [D loss: 0.996508, acc: 0.500000]  [A loss: 0.321721, acc: 1.000000]\n",
      "73: [D loss: 0.971116, acc: 0.500000]  [A loss: 0.319533, acc: 1.000000]\n",
      "74: [D loss: 0.963246, acc: 0.500000]  [A loss: 0.314706, acc: 1.000000]\n",
      "75: [D loss: 0.970093, acc: 0.500000]  [A loss: 0.310636, acc: 1.000000]\n",
      "76: [D loss: 0.980139, acc: 0.500000]  [A loss: 0.305920, acc: 1.000000]\n",
      "77: [D loss: 0.989386, acc: 0.500000]  [A loss: 0.305921, acc: 1.000000]\n",
      "78: [D loss: 0.996716, acc: 0.500000]  [A loss: 0.309680, acc: 1.000000]\n",
      "79: [D loss: 0.985272, acc: 0.500000]  [A loss: 0.310945, acc: 1.000000]\n",
      "80: [D loss: 0.976350, acc: 0.500000]  [A loss: 0.308509, acc: 1.000000]\n",
      "81: [D loss: 0.977735, acc: 0.500000]  [A loss: 0.307975, acc: 1.000000]\n",
      "82: [D loss: 0.977023, acc: 0.500000]  [A loss: 0.301469, acc: 1.000000]\n",
      "83: [D loss: 0.980797, acc: 0.500000]  [A loss: 0.296690, acc: 1.000000]\n",
      "84: [D loss: 0.990857, acc: 0.500000]  [A loss: 0.297896, acc: 1.000000]\n",
      "85: [D loss: 0.989556, acc: 0.500000]  [A loss: 0.299098, acc: 1.000000]\n",
      "86: [D loss: 0.980726, acc: 0.500000]  [A loss: 0.300218, acc: 1.000000]\n",
      "87: [D loss: 0.984456, acc: 0.500000]  [A loss: 0.298967, acc: 1.000000]\n",
      "88: [D loss: 0.986395, acc: 0.500000]  [A loss: 0.298101, acc: 1.000000]\n",
      "89: [D loss: 0.986345, acc: 0.500000]  [A loss: 0.297195, acc: 1.000000]\n",
      "90: [D loss: 0.985123, acc: 0.500000]  [A loss: 0.297202, acc: 1.000000]\n",
      "91: [D loss: 0.986075, acc: 0.500000]  [A loss: 0.297154, acc: 1.000000]\n",
      "92: [D loss: 0.989681, acc: 0.500000]  [A loss: 0.295930, acc: 1.000000]\n",
      "93: [D loss: 0.987816, acc: 0.500000]  [A loss: 0.295314, acc: 1.000000]\n",
      "94: [D loss: 0.990729, acc: 0.500000]  [A loss: 0.294913, acc: 1.000000]\n",
      "95: [D loss: 0.992569, acc: 0.500000]  [A loss: 0.294822, acc: 1.000000]\n",
      "96: [D loss: 0.987508, acc: 0.500000]  [A loss: 0.293739, acc: 1.000000]\n",
      "97: [D loss: 0.990937, acc: 0.500000]  [A loss: 0.292005, acc: 1.000000]\n",
      "98: [D loss: 0.993486, acc: 0.500000]  [A loss: 0.290787, acc: 1.000000]\n",
      "99: [D loss: 1.003115, acc: 0.500000]  [A loss: 0.290016, acc: 1.000000]\n",
      "100: [D loss: 0.993750, acc: 0.500000]  [A loss: 0.291729, acc: 1.000000]\n",
      "101: [D loss: 0.999510, acc: 0.500000]  [A loss: 0.291590, acc: 1.000000]\n",
      "102: [D loss: 0.997354, acc: 0.500000]  [A loss: 0.295604, acc: 1.000000]\n",
      "103: [D loss: 0.986263, acc: 0.500000]  [A loss: 0.295466, acc: 1.000000]\n",
      "104: [D loss: 0.987667, acc: 0.500000]  [A loss: 0.292351, acc: 1.000000]\n",
      "105: [D loss: 0.998140, acc: 0.500000]  [A loss: 0.289602, acc: 1.000000]\n",
      "106: [D loss: 0.997733, acc: 0.500000]  [A loss: 0.286252, acc: 1.000000]\n",
      "107: [D loss: 1.009447, acc: 0.500000]  [A loss: 0.285105, acc: 1.000000]\n",
      "108: [D loss: 1.003061, acc: 0.500000]  [A loss: 0.285130, acc: 1.000000]\n",
      "109: [D loss: 1.012084, acc: 0.500000]  [A loss: 0.285402, acc: 1.000000]\n",
      "110: [D loss: 1.006293, acc: 0.500000]  [A loss: 0.287889, acc: 1.000000]\n",
      "111: [D loss: 1.000096, acc: 0.500000]  [A loss: 0.287040, acc: 1.000000]\n",
      "112: [D loss: 0.998139, acc: 0.500000]  [A loss: 0.287867, acc: 1.000000]\n",
      "113: [D loss: 0.996785, acc: 0.500000]  [A loss: 0.287197, acc: 1.000000]\n",
      "114: [D loss: 1.004025, acc: 0.500000]  [A loss: 0.284047, acc: 1.000000]\n",
      "115: [D loss: 1.003237, acc: 0.500000]  [A loss: 0.284787, acc: 1.000000]\n",
      "116: [D loss: 0.998255, acc: 0.500000]  [A loss: 0.283424, acc: 1.000000]\n",
      "117: [D loss: 1.005785, acc: 0.500000]  [A loss: 0.282402, acc: 1.000000]\n",
      "118: [D loss: 1.003832, acc: 0.500000]  [A loss: 0.283548, acc: 1.000000]\n",
      "119: [D loss: 1.005962, acc: 0.500000]  [A loss: 0.282148, acc: 1.000000]\n",
      "120: [D loss: 1.001017, acc: 0.500000]  [A loss: 0.282915, acc: 1.000000]\n",
      "121: [D loss: 1.002001, acc: 0.500000]  [A loss: 0.280665, acc: 1.000000]\n",
      "122: [D loss: 0.997643, acc: 0.500000]  [A loss: 0.280226, acc: 1.000000]\n",
      "123: [D loss: 1.017779, acc: 0.500000]  [A loss: 0.282724, acc: 1.000000]\n",
      "124: [D loss: 0.992694, acc: 0.500000]  [A loss: 0.281852, acc: 1.000000]\n",
      "125: [D loss: 1.004741, acc: 0.500000]  [A loss: 0.283455, acc: 1.000000]\n",
      "126: [D loss: 1.001986, acc: 0.500000]  [A loss: 0.282401, acc: 1.000000]\n",
      "127: [D loss: 1.002100, acc: 0.500000]  [A loss: 0.282774, acc: 1.000000]\n",
      "128: [D loss: 1.000894, acc: 0.500000]  [A loss: 0.280199, acc: 1.000000]\n",
      "129: [D loss: 1.010435, acc: 0.500000]  [A loss: 0.281449, acc: 1.000000]\n",
      "130: [D loss: 1.002339, acc: 0.500000]  [A loss: 0.277502, acc: 1.000000]\n",
      "131: [D loss: 1.007119, acc: 0.500000]  [A loss: 0.278141, acc: 1.000000]\n",
      "132: [D loss: 1.012740, acc: 0.500000]  [A loss: 0.279791, acc: 1.000000]\n",
      "133: [D loss: 1.001951, acc: 0.500000]  [A loss: 0.280460, acc: 1.000000]\n",
      "134: [D loss: 1.005577, acc: 0.500000]  [A loss: 0.277614, acc: 1.000000]\n",
      "135: [D loss: 1.013007, acc: 0.500000]  [A loss: 0.281115, acc: 1.000000]\n",
      "136: [D loss: 1.004698, acc: 0.500000]  [A loss: 0.279666, acc: 1.000000]\n",
      "137: [D loss: 1.002539, acc: 0.500000]  [A loss: 0.276766, acc: 1.000000]\n",
      "138: [D loss: 1.003517, acc: 0.500000]  [A loss: 0.276522, acc: 1.000000]\n",
      "139: [D loss: 1.007583, acc: 0.500000]  [A loss: 0.275171, acc: 1.000000]\n",
      "140: [D loss: 1.007267, acc: 0.500000]  [A loss: 0.273351, acc: 1.000000]\n",
      "141: [D loss: 1.012978, acc: 0.500000]  [A loss: 0.274498, acc: 1.000000]\n",
      "142: [D loss: 1.014199, acc: 0.500000]  [A loss: 0.271352, acc: 1.000000]\n",
      "143: [D loss: 1.013172, acc: 0.500000]  [A loss: 0.272848, acc: 1.000000]\n",
      "144: [D loss: 1.014482, acc: 0.500000]  [A loss: 0.273001, acc: 1.000000]\n",
      "145: [D loss: 1.009970, acc: 0.500000]  [A loss: 0.271850, acc: 1.000000]\n",
      "146: [D loss: 1.010129, acc: 0.500000]  [A loss: 0.272889, acc: 1.000000]\n",
      "147: [D loss: 1.002437, acc: 0.500000]  [A loss: 0.270005, acc: 1.000000]\n",
      "148: [D loss: 1.016019, acc: 0.500000]  [A loss: 0.273380, acc: 1.000000]\n",
      "149: [D loss: 1.008960, acc: 0.500000]  [A loss: 0.271475, acc: 1.000000]\n",
      "150: [D loss: 1.011816, acc: 0.500000]  [A loss: 0.270249, acc: 1.000000]\n",
      "151: [D loss: 1.008010, acc: 0.500000]  [A loss: 0.269829, acc: 1.000000]\n",
      "152: [D loss: 1.019060, acc: 0.500000]  [A loss: 0.273863, acc: 1.000000]\n",
      "153: [D loss: 1.003077, acc: 0.500000]  [A loss: 0.272015, acc: 1.000000]\n",
      "154: [D loss: 1.008264, acc: 0.500000]  [A loss: 0.268284, acc: 1.000000]\n",
      "155: [D loss: 1.011413, acc: 0.500000]  [A loss: 0.272880, acc: 1.000000]\n",
      "156: [D loss: 1.006219, acc: 0.500000]  [A loss: 0.270751, acc: 1.000000]\n",
      "157: [D loss: 1.010632, acc: 0.500000]  [A loss: 0.270732, acc: 1.000000]\n",
      "158: [D loss: 1.005538, acc: 0.500000]  [A loss: 0.268203, acc: 1.000000]\n",
      "159: [D loss: 1.008515, acc: 0.500000]  [A loss: 0.267175, acc: 1.000000]\n",
      "160: [D loss: 1.023561, acc: 0.500000]  [A loss: 0.268230, acc: 1.000000]\n",
      "161: [D loss: 1.010803, acc: 0.500000]  [A loss: 0.269473, acc: 1.000000]\n",
      "162: [D loss: 1.005112, acc: 0.500000]  [A loss: 0.268400, acc: 1.000000]\n",
      "163: [D loss: 1.016145, acc: 0.500000]  [A loss: 0.264192, acc: 1.000000]\n",
      "164: [D loss: 1.020352, acc: 0.500000]  [A loss: 0.270523, acc: 1.000000]\n",
      "165: [D loss: 1.011730, acc: 0.500000]  [A loss: 0.272068, acc: 1.000000]\n",
      "166: [D loss: 1.014774, acc: 0.500000]  [A loss: 0.273030, acc: 1.000000]\n",
      "167: [D loss: 1.003608, acc: 0.500000]  [A loss: 0.271269, acc: 1.000000]\n",
      "168: [D loss: 1.011207, acc: 0.500000]  [A loss: 0.272205, acc: 1.000000]\n",
      "169: [D loss: 1.007882, acc: 0.500000]  [A loss: 0.270471, acc: 1.000000]\n",
      "170: [D loss: 1.003891, acc: 0.500000]  [A loss: 0.268337, acc: 1.000000]\n",
      "171: [D loss: 1.007668, acc: 0.500000]  [A loss: 0.264177, acc: 1.000000]\n",
      "172: [D loss: 1.029466, acc: 0.500000]  [A loss: 0.267957, acc: 1.000000]\n",
      "173: [D loss: 1.013987, acc: 0.500000]  [A loss: 0.267974, acc: 1.000000]\n",
      "174: [D loss: 1.008703, acc: 0.500000]  [A loss: 0.270662, acc: 1.000000]\n",
      "175: [D loss: 1.004982, acc: 0.500000]  [A loss: 0.267988, acc: 1.000000]\n",
      "176: [D loss: 1.011676, acc: 0.500000]  [A loss: 0.263656, acc: 1.000000]\n",
      "177: [D loss: 1.015504, acc: 0.500000]  [A loss: 0.264986, acc: 1.000000]\n",
      "178: [D loss: 1.017989, acc: 0.500000]  [A loss: 0.266170, acc: 1.000000]\n",
      "179: [D loss: 1.010912, acc: 0.500000]  [A loss: 0.263752, acc: 1.000000]\n",
      "180: [D loss: 1.005605, acc: 0.500000]  [A loss: 0.264695, acc: 1.000000]\n",
      "181: [D loss: 1.018051, acc: 0.500000]  [A loss: 0.268336, acc: 1.000000]\n",
      "182: [D loss: 1.011462, acc: 0.500000]  [A loss: 0.271690, acc: 1.000000]\n",
      "183: [D loss: 1.010171, acc: 0.500000]  [A loss: 0.266022, acc: 1.000000]\n",
      "184: [D loss: 1.005011, acc: 0.500000]  [A loss: 0.265127, acc: 1.000000]\n",
      "185: [D loss: 1.012649, acc: 0.500000]  [A loss: 0.262154, acc: 1.000000]\n",
      "186: [D loss: 1.017574, acc: 0.500000]  [A loss: 0.261159, acc: 1.000000]\n",
      "187: [D loss: 1.015156, acc: 0.500000]  [A loss: 0.263430, acc: 1.000000]\n",
      "188: [D loss: 1.015214, acc: 0.500000]  [A loss: 0.265601, acc: 1.000000]\n",
      "189: [D loss: 1.020490, acc: 0.500000]  [A loss: 0.267407, acc: 1.000000]\n",
      "190: [D loss: 1.012176, acc: 0.500000]  [A loss: 0.266772, acc: 1.000000]\n",
      "191: [D loss: 1.011582, acc: 0.500000]  [A loss: 0.269407, acc: 1.000000]\n",
      "192: [D loss: 1.012255, acc: 0.500000]  [A loss: 0.265718, acc: 1.000000]\n",
      "193: [D loss: 1.007745, acc: 0.500000]  [A loss: 0.262656, acc: 1.000000]\n",
      "194: [D loss: 1.011535, acc: 0.500000]  [A loss: 0.260952, acc: 1.000000]\n",
      "195: [D loss: 1.015671, acc: 0.500000]  [A loss: 0.260801, acc: 1.000000]\n",
      "196: [D loss: 1.012566, acc: 0.500000]  [A loss: 0.259291, acc: 1.000000]\n",
      "197: [D loss: 1.011778, acc: 0.500000]  [A loss: 0.261453, acc: 1.000000]\n",
      "198: [D loss: 1.014804, acc: 0.500000]  [A loss: 0.263045, acc: 1.000000]\n",
      "199: [D loss: 1.014329, acc: 0.500000]  [A loss: 0.261822, acc: 1.000000]\n",
      "200: [D loss: 1.011983, acc: 0.500000]  [A loss: 0.264353, acc: 1.000000]\n",
      "201: [D loss: 1.008054, acc: 0.500000]  [A loss: 0.259229, acc: 1.000000]\n",
      "202: [D loss: 1.015036, acc: 0.500000]  [A loss: 0.257972, acc: 1.000000]\n",
      "203: [D loss: 1.020720, acc: 0.500000]  [A loss: 0.259899, acc: 1.000000]\n",
      "204: [D loss: 1.018909, acc: 0.500000]  [A loss: 0.259725, acc: 1.000000]\n",
      "205: [D loss: 1.015626, acc: 0.500000]  [A loss: 0.255599, acc: 1.000000]\n",
      "206: [D loss: 1.013124, acc: 0.500000]  [A loss: 0.262539, acc: 1.000000]\n",
      "207: [D loss: 1.008824, acc: 0.500000]  [A loss: 0.263391, acc: 1.000000]\n",
      "208: [D loss: 1.019848, acc: 0.500000]  [A loss: 0.261610, acc: 1.000000]\n",
      "209: [D loss: 1.019311, acc: 0.500000]  [A loss: 0.262376, acc: 1.000000]\n",
      "210: [D loss: 1.012663, acc: 0.500000]  [A loss: 0.259220, acc: 1.000000]\n",
      "211: [D loss: 1.011221, acc: 0.500000]  [A loss: 0.266579, acc: 1.000000]\n",
      "212: [D loss: 1.013940, acc: 0.500000]  [A loss: 0.260834, acc: 1.000000]\n",
      "213: [D loss: 1.015773, acc: 0.500000]  [A loss: 0.259966, acc: 1.000000]\n",
      "214: [D loss: 1.019230, acc: 0.500000]  [A loss: 0.259882, acc: 1.000000]\n",
      "215: [D loss: 1.014618, acc: 0.500000]  [A loss: 0.257877, acc: 1.000000]\n",
      "216: [D loss: 1.000952, acc: 0.500000]  [A loss: 0.258597, acc: 1.000000]\n",
      "217: [D loss: 1.015811, acc: 0.500000]  [A loss: 0.256698, acc: 1.000000]\n",
      "218: [D loss: 1.026367, acc: 0.500000]  [A loss: 0.256184, acc: 1.000000]\n",
      "219: [D loss: 1.037427, acc: 0.500000]  [A loss: 0.261039, acc: 1.000000]\n",
      "220: [D loss: 1.021049, acc: 0.500000]  [A loss: 0.265167, acc: 1.000000]\n",
      "221: [D loss: 1.010417, acc: 0.500000]  [A loss: 0.264781, acc: 1.000000]\n",
      "222: [D loss: 1.018986, acc: 0.500000]  [A loss: 0.265022, acc: 1.000000]\n",
      "223: [D loss: 1.006021, acc: 0.500000]  [A loss: 0.262825, acc: 1.000000]\n",
      "224: [D loss: 1.004050, acc: 0.500000]  [A loss: 0.259857, acc: 1.000000]\n",
      "225: [D loss: 1.011597, acc: 0.500000]  [A loss: 0.255371, acc: 1.000000]\n",
      "226: [D loss: 1.022050, acc: 0.500000]  [A loss: 0.258351, acc: 1.000000]\n",
      "227: [D loss: 1.032557, acc: 0.500000]  [A loss: 0.261373, acc: 1.000000]\n",
      "228: [D loss: 1.018085, acc: 0.500000]  [A loss: 0.260626, acc: 1.000000]\n",
      "229: [D loss: 1.009845, acc: 0.500000]  [A loss: 0.262159, acc: 1.000000]\n",
      "230: [D loss: 1.013897, acc: 0.500000]  [A loss: 0.259082, acc: 1.000000]\n",
      "231: [D loss: 1.015033, acc: 0.500000]  [A loss: 0.255797, acc: 1.000000]\n",
      "232: [D loss: 1.014866, acc: 0.500000]  [A loss: 0.255246, acc: 1.000000]\n",
      "233: [D loss: 1.022580, acc: 0.500000]  [A loss: 0.255210, acc: 1.000000]\n",
      "234: [D loss: 1.021923, acc: 0.500000]  [A loss: 0.256267, acc: 1.000000]\n",
      "235: [D loss: 1.020167, acc: 0.500000]  [A loss: 0.259300, acc: 1.000000]\n",
      "236: [D loss: 1.017405, acc: 0.500000]  [A loss: 0.259540, acc: 1.000000]\n",
      "237: [D loss: 1.015831, acc: 0.500000]  [A loss: 0.255623, acc: 1.000000]\n",
      "238: [D loss: 1.013332, acc: 0.500000]  [A loss: 0.255118, acc: 1.000000]\n",
      "239: [D loss: 1.016932, acc: 0.500000]  [A loss: 0.253642, acc: 1.000000]\n",
      "240: [D loss: 1.015132, acc: 0.500000]  [A loss: 0.253520, acc: 1.000000]\n",
      "241: [D loss: 1.022155, acc: 0.500000]  [A loss: 0.253519, acc: 1.000000]\n",
      "242: [D loss: 1.022217, acc: 0.500000]  [A loss: 0.253741, acc: 1.000000]\n",
      "243: [D loss: 1.017022, acc: 0.500000]  [A loss: 0.251431, acc: 1.000000]\n",
      "244: [D loss: 1.019105, acc: 0.500000]  [A loss: 0.254266, acc: 1.000000]\n",
      "245: [D loss: 1.016272, acc: 0.500000]  [A loss: 0.254664, acc: 1.000000]\n",
      "246: [D loss: 1.014912, acc: 0.500000]  [A loss: 0.255708, acc: 1.000000]\n",
      "247: [D loss: 1.011877, acc: 0.500000]  [A loss: 0.253191, acc: 1.000000]\n",
      "248: [D loss: 1.011703, acc: 0.500000]  [A loss: 0.251178, acc: 1.000000]\n",
      "249: [D loss: 1.019473, acc: 0.500000]  [A loss: 0.251442, acc: 1.000000]\n",
      "250: [D loss: 1.017386, acc: 0.500000]  [A loss: 0.252305, acc: 1.000000]\n",
      "251: [D loss: 1.022247, acc: 0.500000]  [A loss: 0.250191, acc: 1.000000]\n",
      "252: [D loss: 1.021195, acc: 0.500000]  [A loss: 0.253059, acc: 1.000000]\n",
      "253: [D loss: 1.025182, acc: 0.500000]  [A loss: 0.256148, acc: 1.000000]\n",
      "254: [D loss: 1.022787, acc: 0.500000]  [A loss: 0.256368, acc: 1.000000]\n",
      "255: [D loss: 1.021745, acc: 0.500000]  [A loss: 0.256950, acc: 1.000000]\n",
      "256: [D loss: 1.029183, acc: 0.500000]  [A loss: 0.262429, acc: 1.000000]\n",
      "257: [D loss: 1.011191, acc: 0.500000]  [A loss: 0.261403, acc: 1.000000]\n",
      "258: [D loss: 1.001055, acc: 0.500000]  [A loss: 0.259604, acc: 1.000000]\n",
      "259: [D loss: 1.004989, acc: 0.500000]  [A loss: 0.255893, acc: 1.000000]\n",
      "260: [D loss: 1.014514, acc: 0.500000]  [A loss: 0.258551, acc: 1.000000]\n",
      "261: [D loss: 1.014079, acc: 0.500000]  [A loss: 0.251786, acc: 1.000000]\n",
      "262: [D loss: 1.022570, acc: 0.500000]  [A loss: 0.254550, acc: 1.000000]\n",
      "263: [D loss: 1.007784, acc: 0.500000]  [A loss: 0.254658, acc: 1.000000]\n",
      "264: [D loss: 1.011509, acc: 0.500000]  [A loss: 0.250754, acc: 1.000000]\n",
      "265: [D loss: 1.033297, acc: 0.500000]  [A loss: 0.245237, acc: 1.000000]\n",
      "266: [D loss: 1.040236, acc: 0.500000]  [A loss: 0.252306, acc: 1.000000]\n",
      "267: [D loss: 1.016803, acc: 0.500000]  [A loss: 0.246590, acc: 1.000000]\n",
      "268: [D loss: 1.027730, acc: 0.500000]  [A loss: 0.249283, acc: 1.000000]\n",
      "269: [D loss: 1.029118, acc: 0.500000]  [A loss: 0.253117, acc: 1.000000]\n",
      "270: [D loss: 1.019653, acc: 0.500000]  [A loss: 0.252680, acc: 1.000000]\n",
      "271: [D loss: 1.013197, acc: 0.500000]  [A loss: 0.250503, acc: 1.000000]\n",
      "272: [D loss: 1.014028, acc: 0.500000]  [A loss: 0.250539, acc: 1.000000]\n",
      "273: [D loss: 1.022709, acc: 0.500000]  [A loss: 0.251353, acc: 1.000000]\n",
      "274: [D loss: 1.017760, acc: 0.500000]  [A loss: 0.248922, acc: 1.000000]\n",
      "275: [D loss: 1.021966, acc: 0.500000]  [A loss: 0.251428, acc: 1.000000]\n",
      "276: [D loss: 1.021497, acc: 0.500000]  [A loss: 0.252305, acc: 1.000000]\n",
      "277: [D loss: 1.025194, acc: 0.500000]  [A loss: 0.254466, acc: 1.000000]\n",
      "278: [D loss: 1.015545, acc: 0.500000]  [A loss: 0.253661, acc: 1.000000]\n",
      "279: [D loss: 1.015216, acc: 0.500000]  [A loss: 0.252605, acc: 1.000000]\n",
      "280: [D loss: 1.011735, acc: 0.500000]  [A loss: 0.252341, acc: 1.000000]\n",
      "281: [D loss: 1.010631, acc: 0.500000]  [A loss: 0.250367, acc: 1.000000]\n",
      "282: [D loss: 1.014826, acc: 0.500000]  [A loss: 0.246221, acc: 1.000000]\n",
      "283: [D loss: 1.027673, acc: 0.500000]  [A loss: 0.244482, acc: 1.000000]\n",
      "284: [D loss: 1.036973, acc: 0.500000]  [A loss: 0.247604, acc: 1.000000]\n",
      "285: [D loss: 1.024250, acc: 0.500000]  [A loss: 0.250188, acc: 1.000000]\n",
      "286: [D loss: 1.022895, acc: 0.500000]  [A loss: 0.253615, acc: 1.000000]\n",
      "287: [D loss: 1.015149, acc: 0.500000]  [A loss: 0.252613, acc: 1.000000]\n",
      "288: [D loss: 1.011236, acc: 0.500000]  [A loss: 0.250796, acc: 1.000000]\n",
      "289: [D loss: 1.013026, acc: 0.500000]  [A loss: 0.247683, acc: 1.000000]\n",
      "290: [D loss: 1.017226, acc: 0.500000]  [A loss: 0.247339, acc: 1.000000]\n",
      "291: [D loss: 1.018916, acc: 0.500000]  [A loss: 0.246615, acc: 1.000000]\n",
      "292: [D loss: 1.026206, acc: 0.500000]  [A loss: 0.243982, acc: 1.000000]\n",
      "293: [D loss: 1.015823, acc: 0.500000]  [A loss: 0.244895, acc: 1.000000]\n",
      "294: [D loss: 1.025203, acc: 0.500000]  [A loss: 0.245390, acc: 1.000000]\n",
      "295: [D loss: 1.022173, acc: 0.500000]  [A loss: 0.246680, acc: 1.000000]\n",
      "296: [D loss: 1.029262, acc: 0.500000]  [A loss: 0.250718, acc: 1.000000]\n",
      "297: [D loss: 1.010670, acc: 0.500000]  [A loss: 0.247741, acc: 1.000000]\n",
      "298: [D loss: 1.011337, acc: 0.500000]  [A loss: 0.248922, acc: 1.000000]\n",
      "299: [D loss: 1.013273, acc: 0.500000]  [A loss: 0.246966, acc: 1.000000]\n",
      "300: [D loss: 1.011390, acc: 0.500000]  [A loss: 0.248593, acc: 1.000000]\n",
      "301: [D loss: 1.024786, acc: 0.500000]  [A loss: 0.242500, acc: 1.000000]\n",
      "302: [D loss: 1.023017, acc: 0.500000]  [A loss: 0.243319, acc: 1.000000]\n",
      "303: [D loss: 1.032426, acc: 0.500000]  [A loss: 0.243376, acc: 1.000000]\n",
      "304: [D loss: 1.027046, acc: 0.500000]  [A loss: 0.244899, acc: 1.000000]\n",
      "305: [D loss: 1.028517, acc: 0.500000]  [A loss: 0.243521, acc: 1.000000]\n",
      "306: [D loss: 1.028288, acc: 0.500000]  [A loss: 0.244935, acc: 1.000000]\n",
      "307: [D loss: 1.028465, acc: 0.500000]  [A loss: 0.247626, acc: 1.000000]\n",
      "308: [D loss: 1.016251, acc: 0.500000]  [A loss: 0.249193, acc: 1.000000]\n",
      "309: [D loss: 1.015872, acc: 0.500000]  [A loss: 0.248921, acc: 1.000000]\n",
      "310: [D loss: 1.016884, acc: 0.500000]  [A loss: 0.249576, acc: 1.000000]\n",
      "311: [D loss: 1.015468, acc: 0.500000]  [A loss: 0.251932, acc: 1.000000]\n",
      "312: [D loss: 1.010093, acc: 0.500000]  [A loss: 0.247379, acc: 1.000000]\n",
      "313: [D loss: 1.010521, acc: 0.500000]  [A loss: 0.245417, acc: 1.000000]\n",
      "314: [D loss: 1.015523, acc: 0.500000]  [A loss: 0.247494, acc: 1.000000]\n",
      "315: [D loss: 1.016429, acc: 0.500000]  [A loss: 0.246157, acc: 1.000000]\n",
      "316: [D loss: 1.025093, acc: 0.500000]  [A loss: 0.243696, acc: 1.000000]\n",
      "317: [D loss: 1.021110, acc: 0.500000]  [A loss: 0.244017, acc: 1.000000]\n",
      "318: [D loss: 1.016933, acc: 0.500000]  [A loss: 0.243373, acc: 1.000000]\n",
      "319: [D loss: 1.016700, acc: 0.500000]  [A loss: 0.245208, acc: 1.000000]\n",
      "320: [D loss: 1.016778, acc: 0.500000]  [A loss: 0.242666, acc: 1.000000]\n",
      "321: [D loss: 1.012957, acc: 0.500000]  [A loss: 0.243560, acc: 1.000000]\n",
      "322: [D loss: 1.018207, acc: 0.500000]  [A loss: 0.241952, acc: 1.000000]\n",
      "323: [D loss: 1.017000, acc: 0.500000]  [A loss: 0.245857, acc: 1.000000]\n",
      "324: [D loss: 1.017148, acc: 0.500000]  [A loss: 0.241085, acc: 1.000000]\n",
      "325: [D loss: 1.023286, acc: 0.500000]  [A loss: 0.240325, acc: 1.000000]\n",
      "326: [D loss: 1.018556, acc: 0.500000]  [A loss: 0.244086, acc: 1.000000]\n",
      "327: [D loss: 1.023287, acc: 0.500000]  [A loss: 0.240868, acc: 1.000000]\n",
      "328: [D loss: 1.020601, acc: 0.500000]  [A loss: 0.243028, acc: 1.000000]\n",
      "329: [D loss: 1.012570, acc: 0.500000]  [A loss: 0.242070, acc: 1.000000]\n",
      "330: [D loss: 1.012304, acc: 0.500000]  [A loss: 0.241785, acc: 1.000000]\n",
      "331: [D loss: 1.020216, acc: 0.500000]  [A loss: 0.240906, acc: 1.000000]\n",
      "332: [D loss: 1.018905, acc: 0.500000]  [A loss: 0.243040, acc: 1.000000]\n",
      "333: [D loss: 1.023920, acc: 0.500000]  [A loss: 0.243608, acc: 1.000000]\n",
      "334: [D loss: 1.011763, acc: 0.500000]  [A loss: 0.242773, acc: 1.000000]\n",
      "335: [D loss: 1.022223, acc: 0.500000]  [A loss: 0.244387, acc: 1.000000]\n",
      "336: [D loss: 1.014911, acc: 0.500000]  [A loss: 0.244933, acc: 1.000000]\n",
      "337: [D loss: 1.018830, acc: 0.500000]  [A loss: 0.243580, acc: 1.000000]\n",
      "338: [D loss: 1.021941, acc: 0.500000]  [A loss: 0.247418, acc: 1.000000]\n",
      "339: [D loss: 1.023717, acc: 0.500000]  [A loss: 0.245991, acc: 1.000000]\n",
      "340: [D loss: 1.021166, acc: 0.500000]  [A loss: 0.248952, acc: 1.000000]\n",
      "341: [D loss: 1.010688, acc: 0.500000]  [A loss: 0.247152, acc: 1.000000]\n",
      "342: [D loss: 1.018799, acc: 0.500000]  [A loss: 0.245579, acc: 1.000000]\n",
      "343: [D loss: 1.026879, acc: 0.500000]  [A loss: 0.246458, acc: 1.000000]\n",
      "344: [D loss: 1.032738, acc: 0.500000]  [A loss: 0.248114, acc: 1.000000]\n",
      "345: [D loss: 1.021385, acc: 0.500000]  [A loss: 0.247173, acc: 1.000000]\n",
      "346: [D loss: 1.012830, acc: 0.500000]  [A loss: 0.249997, acc: 1.000000]\n",
      "347: [D loss: 1.019426, acc: 0.500000]  [A loss: 0.247780, acc: 1.000000]\n",
      "348: [D loss: 1.013696, acc: 0.500000]  [A loss: 0.245388, acc: 1.000000]\n",
      "349: [D loss: 1.018106, acc: 0.500000]  [A loss: 0.247397, acc: 1.000000]\n",
      "350: [D loss: 1.017145, acc: 0.500000]  [A loss: 0.246768, acc: 1.000000]\n",
      "351: [D loss: 1.018631, acc: 0.500000]  [A loss: 0.241813, acc: 1.000000]\n",
      "352: [D loss: 1.017588, acc: 0.500000]  [A loss: 0.247118, acc: 1.000000]\n",
      "353: [D loss: 1.017670, acc: 0.500000]  [A loss: 0.241457, acc: 1.000000]\n",
      "354: [D loss: 1.022744, acc: 0.500000]  [A loss: 0.240987, acc: 1.000000]\n",
      "355: [D loss: 1.018861, acc: 0.500000]  [A loss: 0.243318, acc: 1.000000]\n",
      "356: [D loss: 1.021441, acc: 0.500000]  [A loss: 0.243334, acc: 1.000000]\n",
      "357: [D loss: 1.023414, acc: 0.500000]  [A loss: 0.242637, acc: 1.000000]\n",
      "358: [D loss: 1.020352, acc: 0.500000]  [A loss: 0.246266, acc: 1.000000]\n",
      "359: [D loss: 1.017298, acc: 0.500000]  [A loss: 0.239462, acc: 1.000000]\n",
      "360: [D loss: 1.019265, acc: 0.500000]  [A loss: 0.244185, acc: 1.000000]\n",
      "361: [D loss: 1.016165, acc: 0.500000]  [A loss: 0.244804, acc: 1.000000]\n",
      "362: [D loss: 1.020499, acc: 0.500000]  [A loss: 0.238560, acc: 1.000000]\n",
      "363: [D loss: 1.030315, acc: 0.500000]  [A loss: 0.242497, acc: 1.000000]\n",
      "364: [D loss: 1.024806, acc: 0.500000]  [A loss: 0.243429, acc: 1.000000]\n",
      "365: [D loss: 1.014473, acc: 0.500000]  [A loss: 0.242189, acc: 1.000000]\n",
      "366: [D loss: 1.020623, acc: 0.500000]  [A loss: 0.239996, acc: 1.000000]\n",
      "367: [D loss: 1.022614, acc: 0.500000]  [A loss: 0.240222, acc: 1.000000]\n",
      "368: [D loss: 1.023812, acc: 0.500000]  [A loss: 0.241101, acc: 1.000000]\n",
      "369: [D loss: 1.024764, acc: 0.500000]  [A loss: 0.242238, acc: 1.000000]\n",
      "370: [D loss: 1.023082, acc: 0.500000]  [A loss: 0.240315, acc: 1.000000]\n",
      "371: [D loss: 1.021822, acc: 0.500000]  [A loss: 0.242065, acc: 1.000000]\n",
      "372: [D loss: 1.021966, acc: 0.500000]  [A loss: 0.240192, acc: 1.000000]\n",
      "373: [D loss: 1.022038, acc: 0.500000]  [A loss: 0.240601, acc: 1.000000]\n",
      "374: [D loss: 1.020210, acc: 0.500000]  [A loss: 0.240880, acc: 1.000000]\n",
      "375: [D loss: 1.020934, acc: 0.500000]  [A loss: 0.236727, acc: 1.000000]\n",
      "376: [D loss: 1.030127, acc: 0.500000]  [A loss: 0.234528, acc: 1.000000]\n",
      "377: [D loss: 1.040942, acc: 0.500000]  [A loss: 0.238693, acc: 1.000000]\n",
      "378: [D loss: 1.043152, acc: 0.500000]  [A loss: 0.244393, acc: 1.000000]\n",
      "379: [D loss: 1.034410, acc: 0.500000]  [A loss: 0.249967, acc: 1.000000]\n",
      "380: [D loss: 1.023074, acc: 0.500000]  [A loss: 0.253754, acc: 1.000000]\n",
      "381: [D loss: 1.015943, acc: 0.500000]  [A loss: 0.250728, acc: 1.000000]\n",
      "382: [D loss: 1.012618, acc: 0.500000]  [A loss: 0.252003, acc: 1.000000]\n",
      "383: [D loss: 1.013597, acc: 0.500000]  [A loss: 0.249506, acc: 1.000000]\n",
      "384: [D loss: 1.015320, acc: 0.500000]  [A loss: 0.250365, acc: 1.000000]\n",
      "385: [D loss: 1.020369, acc: 0.500000]  [A loss: 0.248763, acc: 1.000000]\n",
      "386: [D loss: 1.034826, acc: 0.500000]  [A loss: 0.250510, acc: 1.000000]\n",
      "387: [D loss: 1.031153, acc: 0.500000]  [A loss: 0.244177, acc: 1.000000]\n",
      "388: [D loss: 1.025204, acc: 0.500000]  [A loss: 0.247073, acc: 1.000000]\n",
      "389: [D loss: 1.018831, acc: 0.500000]  [A loss: 0.246421, acc: 1.000000]\n",
      "390: [D loss: 1.017060, acc: 0.500000]  [A loss: 0.245904, acc: 1.000000]\n",
      "391: [D loss: 1.019791, acc: 0.500000]  [A loss: 0.241917, acc: 1.000000]\n",
      "392: [D loss: 1.025463, acc: 0.500000]  [A loss: 0.242876, acc: 1.000000]\n",
      "393: [D loss: 1.025712, acc: 0.500000]  [A loss: 0.242855, acc: 1.000000]\n",
      "394: [D loss: 1.021375, acc: 0.500000]  [A loss: 0.243339, acc: 1.000000]\n",
      "395: [D loss: 1.016258, acc: 0.500000]  [A loss: 0.241308, acc: 1.000000]\n",
      "396: [D loss: 1.018723, acc: 0.500000]  [A loss: 0.239828, acc: 1.000000]\n",
      "397: [D loss: 1.022686, acc: 0.500000]  [A loss: 0.240321, acc: 1.000000]\n",
      "398: [D loss: 1.031491, acc: 0.500000]  [A loss: 0.240050, acc: 1.000000]\n",
      "399: [D loss: 1.030262, acc: 0.500000]  [A loss: 0.239944, acc: 1.000000]\n",
      "400: [D loss: 1.026711, acc: 0.500000]  [A loss: 0.248052, acc: 1.000000]\n",
      "401: [D loss: 1.019640, acc: 0.500000]  [A loss: 0.243369, acc: 1.000000]\n",
      "402: [D loss: 1.021047, acc: 0.500000]  [A loss: 0.246331, acc: 1.000000]\n",
      "403: [D loss: 1.019288, acc: 0.500000]  [A loss: 0.242508, acc: 1.000000]\n",
      "404: [D loss: 1.024355, acc: 0.500000]  [A loss: 0.239222, acc: 1.000000]\n",
      "405: [D loss: 1.028812, acc: 0.500000]  [A loss: 0.238967, acc: 1.000000]\n",
      "406: [D loss: 1.027780, acc: 0.500000]  [A loss: 0.239709, acc: 1.000000]\n",
      "407: [D loss: 1.022782, acc: 0.500000]  [A loss: 0.238973, acc: 1.000000]\n",
      "408: [D loss: 1.018645, acc: 0.500000]  [A loss: 0.236714, acc: 1.000000]\n",
      "409: [D loss: 1.021186, acc: 0.500000]  [A loss: 0.239069, acc: 1.000000]\n",
      "410: [D loss: 1.020369, acc: 0.500000]  [A loss: 0.235808, acc: 1.000000]\n",
      "411: [D loss: 1.023756, acc: 0.500000]  [A loss: 0.237806, acc: 1.000000]\n",
      "412: [D loss: 1.026793, acc: 0.500000]  [A loss: 0.239345, acc: 1.000000]\n",
      "413: [D loss: 1.026328, acc: 0.500000]  [A loss: 0.239319, acc: 1.000000]\n",
      "414: [D loss: 1.026847, acc: 0.500000]  [A loss: 0.239287, acc: 1.000000]\n",
      "415: [D loss: 1.022094, acc: 0.500000]  [A loss: 0.240570, acc: 1.000000]\n",
      "416: [D loss: 1.030502, acc: 0.500000]  [A loss: 0.240935, acc: 1.000000]\n",
      "417: [D loss: 1.022528, acc: 0.500000]  [A loss: 0.242953, acc: 1.000000]\n",
      "418: [D loss: 1.019861, acc: 0.500000]  [A loss: 0.240501, acc: 1.000000]\n",
      "419: [D loss: 1.021054, acc: 0.500000]  [A loss: 0.243558, acc: 1.000000]\n",
      "420: [D loss: 1.023189, acc: 0.500000]  [A loss: 0.241072, acc: 1.000000]\n",
      "421: [D loss: 1.021375, acc: 0.500000]  [A loss: 0.238039, acc: 1.000000]\n",
      "422: [D loss: 1.025339, acc: 0.500000]  [A loss: 0.241044, acc: 1.000000]\n",
      "423: [D loss: 1.021025, acc: 0.500000]  [A loss: 0.239941, acc: 1.000000]\n",
      "424: [D loss: 1.026382, acc: 0.500000]  [A loss: 0.238046, acc: 1.000000]\n",
      "425: [D loss: 1.017208, acc: 0.500000]  [A loss: 0.239313, acc: 1.000000]\n",
      "426: [D loss: 1.018345, acc: 0.500000]  [A loss: 0.237085, acc: 1.000000]\n",
      "427: [D loss: 1.020759, acc: 0.500000]  [A loss: 0.237808, acc: 1.000000]\n",
      "428: [D loss: 1.021118, acc: 0.500000]  [A loss: 0.238095, acc: 1.000000]\n",
      "429: [D loss: 1.028302, acc: 0.500000]  [A loss: 0.238710, acc: 1.000000]\n",
      "430: [D loss: 1.034446, acc: 0.500000]  [A loss: 0.238334, acc: 1.000000]\n",
      "431: [D loss: 1.022702, acc: 0.500000]  [A loss: 0.236585, acc: 1.000000]\n",
      "432: [D loss: 1.019561, acc: 0.500000]  [A loss: 0.238583, acc: 1.000000]\n",
      "433: [D loss: 1.027456, acc: 0.500000]  [A loss: 0.237315, acc: 1.000000]\n",
      "434: [D loss: 1.019766, acc: 0.500000]  [A loss: 0.240147, acc: 1.000000]\n",
      "435: [D loss: 1.021768, acc: 0.500000]  [A loss: 0.237393, acc: 1.000000]\n",
      "436: [D loss: 1.025610, acc: 0.500000]  [A loss: 0.236820, acc: 1.000000]\n",
      "437: [D loss: 1.026376, acc: 0.500000]  [A loss: 0.235699, acc: 1.000000]\n",
      "438: [D loss: 1.026353, acc: 0.500000]  [A loss: 0.234649, acc: 1.000000]\n",
      "439: [D loss: 1.025464, acc: 0.500000]  [A loss: 0.235474, acc: 1.000000]\n",
      "440: [D loss: 1.021921, acc: 0.500000]  [A loss: 0.233320, acc: 1.000000]\n",
      "441: [D loss: 1.032111, acc: 0.500000]  [A loss: 0.237170, acc: 1.000000]\n",
      "442: [D loss: 1.026316, acc: 0.500000]  [A loss: 0.235804, acc: 1.000000]\n",
      "443: [D loss: 1.022548, acc: 0.500000]  [A loss: 0.234997, acc: 1.000000]\n",
      "444: [D loss: 1.028692, acc: 0.500000]  [A loss: 0.232243, acc: 1.000000]\n",
      "445: [D loss: 1.027340, acc: 0.500000]  [A loss: 0.232570, acc: 1.000000]\n",
      "446: [D loss: 1.032775, acc: 0.500000]  [A loss: 0.231448, acc: 1.000000]\n",
      "447: [D loss: 1.031446, acc: 0.500000]  [A loss: 0.233400, acc: 1.000000]\n",
      "448: [D loss: 1.024303, acc: 0.500000]  [A loss: 0.234924, acc: 1.000000]\n",
      "449: [D loss: 1.021248, acc: 0.500000]  [A loss: 0.238668, acc: 1.000000]\n",
      "450: [D loss: 1.013760, acc: 0.500000]  [A loss: 0.236748, acc: 1.000000]\n",
      "451: [D loss: 1.022296, acc: 0.500000]  [A loss: 0.235733, acc: 1.000000]\n",
      "452: [D loss: 1.019596, acc: 0.500000]  [A loss: 0.235897, acc: 1.000000]\n",
      "453: [D loss: 1.023420, acc: 0.500000]  [A loss: 0.235869, acc: 1.000000]\n",
      "454: [D loss: 1.020743, acc: 0.500000]  [A loss: 0.234696, acc: 1.000000]\n",
      "455: [D loss: 1.033376, acc: 0.500000]  [A loss: 0.234555, acc: 1.000000]\n",
      "456: [D loss: 1.020792, acc: 0.500000]  [A loss: 0.230901, acc: 1.000000]\n",
      "457: [D loss: 1.026586, acc: 0.500000]  [A loss: 0.232614, acc: 1.000000]\n",
      "458: [D loss: 1.039574, acc: 0.500000]  [A loss: 0.236525, acc: 1.000000]\n",
      "459: [D loss: 1.032507, acc: 0.500000]  [A loss: 0.233729, acc: 1.000000]\n",
      "460: [D loss: 1.029224, acc: 0.500000]  [A loss: 0.231422, acc: 1.000000]\n",
      "461: [D loss: 1.025855, acc: 0.500000]  [A loss: 0.231744, acc: 1.000000]\n",
      "462: [D loss: 1.028436, acc: 0.500000]  [A loss: 0.230982, acc: 1.000000]\n",
      "463: [D loss: 1.034822, acc: 0.500000]  [A loss: 0.235075, acc: 1.000000]\n",
      "464: [D loss: 1.032480, acc: 0.500000]  [A loss: 0.237046, acc: 1.000000]\n",
      "465: [D loss: 1.022219, acc: 0.500000]  [A loss: 0.236124, acc: 1.000000]\n",
      "466: [D loss: 1.020317, acc: 0.500000]  [A loss: 0.238041, acc: 1.000000]\n",
      "467: [D loss: 1.023397, acc: 0.500000]  [A loss: 0.235564, acc: 1.000000]\n",
      "468: [D loss: 1.023876, acc: 0.500000]  [A loss: 0.237509, acc: 1.000000]\n",
      "469: [D loss: 1.016513, acc: 0.500000]  [A loss: 0.231735, acc: 1.000000]\n",
      "470: [D loss: 1.020153, acc: 0.500000]  [A loss: 0.231445, acc: 1.000000]\n",
      "471: [D loss: 1.026063, acc: 0.500000]  [A loss: 0.234537, acc: 1.000000]\n",
      "472: [D loss: 1.028788, acc: 0.500000]  [A loss: 0.235294, acc: 1.000000]\n",
      "473: [D loss: 1.020268, acc: 0.500000]  [A loss: 0.233013, acc: 1.000000]\n",
      "474: [D loss: 1.023480, acc: 0.500000]  [A loss: 0.232756, acc: 1.000000]\n",
      "475: [D loss: 1.027102, acc: 0.500000]  [A loss: 0.231681, acc: 1.000000]\n",
      "476: [D loss: 1.019854, acc: 0.500000]  [A loss: 0.232103, acc: 1.000000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1acfd48c6941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmnist_dcgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST_DCGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmnist_dcgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmnist_dcgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmnist_dcgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave2file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0d8171874dc3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_steps, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0ma_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madversarial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mlog_mesg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%d: [D loss: %f, acc: %f]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mlog_mesg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s  [A loss: %f, acc: %f]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlog_mesg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\KGAN\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\KGAN\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\KGAN\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\KGAN\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnist_dcgan = MNIST_DCGAN()\n",
    "mnist_dcgan.train(train_steps=2000, batch_size=256, save_interval=100)\n",
    "mnist_dcgan.plot_images(fake=True)\n",
    "mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
